<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>SkyNet Biometric Dashboard</title>
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!-- Astro Font -->
<link href="https://fonts.googleapis.com/css2?family=Audiowide&display=swap" rel="stylesheet"/>

<!-- MathJax -->
<script>
window.MathJax = {
  tex: { inlineMath: [["\\(","\\)"],["$", "$"]] },
  svg: { fontCache: "global" }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

<!-- Chart.js (unchanged UI; defaults configured in script) -->
<script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>

<!-- Face API (CDN) -->
<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

<style>
  /* UI preserved exactly as provided */
  body{
    margin:0;
    background:#000;
    color:#6cf;
    font-family:'Audiowide', sans-serif;
  }
  #container{
    width:100vw; height:100vh;
    display:flex; flex-direction:column;
    align-items:center; justify-content:flex-start;
    /* make container a positioned parent so absolute overlays can be placed reliably */
    position:relative;
    padding:12px 0;
    box-sizing:border-box;
  }
  video{
    width:320px; height:auto;
    border:2px solid #0af;
    border-radius:12px;
    margin-top:10px;
    display:block;
  }
  canvas{
    position:absolute;
    pointer-events:none;
  }

  #metrics{
    margin-top:10px;
    width:90%;
    color:#8cf;
    font-size:14px;
    text-shadow:0 0 8px #0af;
  }

  .eq{
    margin-top:6px;
    padding:6px;
    background:rgba(0,60,120,0.18);
    border-radius:12px;
    border:1px solid rgba(0,140,255,0.4);
  }

  /* Alluvial SVG matches UI aesthetic, no layout changes */
  #alluvialSVG{
    width:90%;
    height:200px;
    margin-top:10px;
    filter: drop-shadow(0 0 6px rgba(0,140,255,0.35));
  }
</style>
</head>
<body>
<div id="container">

  <!-- Front Camera -->
  <video id="camFront" autoplay muted playsinline></video>
  <canvas id="frontOverlay" width="320" height="240"></canvas>

  <!-- Rear Camera -->
  <video id="camRear" autoplay muted playsinline></video>
  <canvas id="rearOverlay" width="320" height="240"></canvas>

  <!-- HUD Metrics (UI unchanged) -->
  <div id="metrics">
    <div>HR Front: <span id="hrFront">60</span> bpm</div>
    <div>HR Rear: <span id="hrRear">60</span> bpm</div>
    <div>PPG: <span id="ppgValue">1</span></div>
    <div>SpO₂: <span id="spo2Value">98</span>%</div>

    <!-- NEW METRIC BLOCK (below SpO2), divided by live PPG -->
    <div id="equationsBlock" class="eq">
      <strong>Eye Metrics (normalized / PPG):</strong>

      <div id="eqGamma">\( \gamma = \frac{I_{max}}{I_{min}} \div PPG \)</div>
      <div id="eqP">\( P = \frac{TP}{TP+FP} \div PPG \)</div>
      <div id="eqDepth">\( \sqrt{D^2 - d^2} \div PPG \)</div>
      <div id="eqHRN">\( \text{NormHR} = \frac{Metric}{HR} \div PPG \)</div>
      <div id="eqLens">\( \frac{1}{f} = \frac{1}{u} + \frac{1}{v} \div PPG \)</div>
      <div id="eqInvSq">\( L \propto \frac{1}{d^2} \div PPG \)</div>
    </div>
  </div>

  <!-- Alluvial Diagram for Iris-Retina Signal (added without altering existing UI elements) -->
  <svg id="alluvialSVG"></svg>

</div>

<script>
/* ===========================================================
   CHART DEFAULTS (thick lines; thermal intent if charts added)
=========================================================== */
if (window.Chart) {
  Chart.defaults.elements.line.borderWidth = 4;
  Chart.defaults.elements.point.radius = 0;
  Chart.defaults.plugins.legend.labels.color = '#8cf';
  Chart.defaults.color = '#8cf';
}

/* ===========================================================
   DOM REFERENCES
=========================================================== */
const camFront = document.getElementById("camFront");
const camRear = document.getElementById("camRear");
const frontOverlay = document.getElementById("frontOverlay");
const rearOverlay = document.getElementById("rearOverlay");
const hrFront = document.getElementById("hrFront");
const hrRear = document.getElementById("hrRear");
const ppgValue = document.getElementById("ppgValue");

const eqGamma = document.getElementById("eqGamma");
const eqP = document.getElementById("eqP");
const eqDepth = document.getElementById("eqDepth");
const eqHRN = document.getElementById("eqHRN");
const eqLens = document.getElementById("eqLens");
const eqInvSq = document.getElementById("eqInvSq");

const alluvialSVG = document.getElementById("alluvialSVG");

/* ===========================================================
   LOAD FACE-API MODELS (ensure /models hosts the model files)
   If you prefer built-in hosted models, change loadFromUri path.
=========================================================== */
async function loadModels(){
  try{
    await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
    await faceapi.nets.faceLandmark68TinyNet.loadFromUri('/models');
    console.log("face-api models loaded");
  }catch(e){
    console.warn("Error loading face-api models from /models:", e);
    // fallback: try CDN-hosted models location if you have one, or notify user
  }
}
loadModels();

/* ===========================================================
   CAMERA START (front + rear) - attempts both cameras
   Uses facingMode hints; device may ignore or allow only one stream.
=========================================================== */
async function startCam(id, facingMode){
  try{
    // Request a constrained stream; browsers decide which camera to supply.
    const constraints = {
      audio: false,
      video: { facingMode: facingMode, width: { ideal: 640 }, height: { ideal: 480 } }
    };
    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const vid = document.getElementById(id);
    vid.srcObject = stream;
    await vid.play().catch(()=>{/* ignore autoplay block */});
    console.log(`Camera started for ${id} (facingMode=${facingMode})`);
    return stream;
  }catch(err){
    console.warn(`Could not start ${id} (${facingMode}):`, err);
    return null;
  }
}

// Try to start both (some devices won't give two simultaneous cameras)
(async ()=>{
  await startCam("camFront","user");
  await startCam("camRear","environment");
})();

/* ===========================================================
   ALIGN OVERLAY CANVAS TO VIDEO (no DOM changes)
   Positions canvas on top of its corresponding video element.
=========================================================== */
function alignOverlay(vid, canvas){
  // If video isn't displayed yet, fallback sizes
  const rect = vid.getBoundingClientRect();
  const computedWidth = vid.videoWidth || rect.width || 320;
  const computedHeight = vid.videoHeight || rect.height || 240;

  canvas.width = Math.round(computedWidth);
  canvas.height = Math.round(computedHeight);

  // Position overlay absolutely within #container (container is relative)
  const containerRect = document.getElementById('container').getBoundingClientRect();
  canvas.style.left = (rect.left - containerRect.left) + "px";
  canvas.style.top  = (rect.top  - containerRect.top) + "px";
  canvas.style.width = rect.width + "px";
  canvas.style.height = rect.height + "px";
}

/* ===========================================================
   SNAPSHOT + BICUBIC SCALE 512×384
=========================================================== */
function getEyeCrop(video){
  const tmp = document.createElement("canvas");
  tmp.width = 512;
  tmp.height = 384;

  const ctx = tmp.getContext("2d");
  ctx.imageSmoothingEnabled = true;
  ctx.imageSmoothingQuality = "high";

  // draw the full video scaled to 512x384 (bicubic-ish via smoothingQuality)
  ctx.drawImage(video, 0, 0, tmp.width, tmp.height);
  return tmp;
}

/* ===========================================================
   MATH — compute eye metrics
=========================================================== */
function computeGamma(imgData){
  let min = 255, max = 0;
  const d = imgData.data;
  for(let i=0;i<d.length;i+=4){
    const v = d[i]; // red channel
    if (v<min) min=v;
    if (v>max) max=v;
  }
  return (max / Math.max(min, 1e-9));
}

function computePrecision(tp, fp){
  return tp / (tp + fp + 1e-9);
}

function computeDepth(D,d){
  // fixed formula: sqrt(D^2 - d^2)
  return Math.sqrt(Math.max(D*D - d*d,0));
}

function thinLens(u,v){
  const denom = (1/u + 1/v);
  return 1 / (denom || 1e-9);
}

function invSq(d){
  return 1/(d*d + 1e-9);
}

/* ===========================================================
   IRONBOW THERMAL COLOR MAP (yellow->orange->red->purple)
=========================================================== */
function ironbowColor(value, min=0, max=1){
  const norm = Math.min(Math.max((value-min)/(max-min), 0), 1);
  const stops = [
    { pct: 0.0, color: [255,255,0] },   // yellow
    { pct: 0.3, color: [255,165,0] },   // orange
    { pct: 0.6, color: [255,0,0] },     // red
    { pct: 1.0, color: [128,0,128] }    // purple
  ];

  for(let i=1;i<stops.length;i++){
    if(norm <= stops[i].pct){
      const prev = stops[i-1], next = stops[i];
      const ratio = (norm - prev.pct)/(next.pct - prev.pct || 1);
      const r = Math.round(prev.color[0] + ratio*(next.color[0]-prev.color[0]));
      const g = Math.round(prev.color[1] + ratio*(next.color[1]-prev.color[1]));
      const b = Math.round(prev.color[2] + ratio*(next.color[2]-prev.color[2]));
      return `rgb(${r},${g},${b})`;
    }
  }
  return "rgb(128,0,128)";
}

/* ===========================================================
   UPDATE EQUATION HTML (always divided by LIVE PPG)
   MathJax-safe innerHTML strings
=========================================================== */
function updateEquations(gamma, prec, depth, hr){
  const ppg = Number(ppgValue.textContent) || 1;  // always live

  const gN = (gamma||0) / ppg;
  const pN = (prec||0)  / ppg;
  const dN = (depth||0) / ppg;

  const lens = thinLens(30, 60);          // placeholder focal calc
  const inv  = invSq(70);                 // placeholder distance

  // Use a single backslash escape in the final HTML: JS string must contain \\( .. \\)
  eqGamma.innerHTML = `\\( \\gamma = ${gN.toFixed(3)} \\)`;
  eqP.innerHTML     = `\\( P = ${pN.toFixed(3)} \\)`;
  eqDepth.innerHTML = `\\( ${dN.toFixed(3)} \\)`;
  eqHRN.innerHTML   = `\\( \\frac{Metric}{${hr}} = ${(1/(hr||1)).toFixed(3)} \\)`;
  eqLens.innerHTML  = `\\( f = ${lens.toFixed(4)} \\)`;
  eqInvSq.innerHTML = `\\( L \\propto ${inv.toFixed(4)} \\)`;

  // Ask MathJax to typeset the new inline formulas
  if (window.MathJax && MathJax.typesetPromise) {
    MathJax.typesetPromise();
  }
}

/* ===========================================================
   ALLUVIAL DIAGRAM (Iris–Retina Signal, thermal-coded)
=========================================================== */
function drawAlluvial(values){
  if (!values || values.length === 0) return;

  // values: array of numbers; normalize for color mapping
  const vMin = Math.min(...values);
  const vMax = Math.max(...values);
  const norm = values.map(v => (v - vMin) / Math.max((vMax - vMin), 1e-9));

  // Clear and draw smooth paths
  while (alluvialSVG.firstChild) alluvialSVG.removeChild(alluvialSVG.firstChild);

  const width = alluvialSVG.clientWidth || alluvialSVG.getBoundingClientRect().width || 320;
  const height = alluvialSVG.clientHeight || alluvialSVG.getBoundingClientRect().height || 200;

  values.forEach((v,i)=>{
    const y = 20 + i * (height / (values.length + 1));
    const x1 = 20, x2 = width - 20;

    const d = `M${x1},${y} C${x1+width*0.3},${y-20} ${x2-width*0.3},${y+20} ${x2},${y}`;
    const path = document.createElementNS("http://www.w3.org/2000/svg","path");
    path.setAttribute("d", d);
    path.setAttribute("stroke", ironbowColor(norm[i], 0, 1));
    path.setAttribute("stroke-width", "6");
    path.setAttribute("fill", "none");
    path.setAttribute("stroke-linecap", "round");
    path.setAttribute("opacity", "0.95");
    alluvialSVG.appendChild(path);
  });
}

/* ===========================================================
   STATE FOR LIVE LOOP
=========================================================== */
const lastMetrics = {
  gamma: 1.0,
  prec:  0.95,
  depth: 8.0,
  hr:    60
};

/* ===========================================================
   MAIN LOOP — detect eyes on both cameras (thermal overlays)
   Integrates the bicubic crop + metrics from Script 1
=========================================================== */
async function eyeLoop(){
  const vids = [
    { vid:camFront, overlay:frontOverlay, hrEl:hrFront },
    { vid:camRear,  overlay:rearOverlay,  hrEl:hrRear  }
  ];

  // Align overlays every frame (keeps them on top)
  vids.forEach(v => alignOverlay(v.vid, v.overlay));

  for(const v of vids){
    if(v.vid && v.vid.readyState >= 2){
      try{
        // Detect face and landmarks (tiny detector for speed)
        const det = await faceapi.detectSingleFace(
          v.vid,
          new faceapi.TinyFaceDetectorOptions()
        ).withFaceLandmarks(true);

        const ctx = v.overlay.getContext("2d");
        ctx.clearRect(0,0,v.overlay.width,v.overlay.height);

        if(det && det.landmarks){
          // Use left eye landmarks (you can also draw right eye similarly)
          const pts = det.landmarks.getLeftEye();

          // Thermal stroke color chosen by current PPG (visual only)
          const ppg = Number(ppgValue.textContent) || 1;
          const thermal = ironbowColor(ppg, 0.5, 2.0);

          ctx.strokeStyle = thermal;
          ctx.lineWidth = Math.max(2, Math.round(v.overlay.width * 0.01));
          ctx.beginPath();
          pts.forEach((p,i)=>{
            // landmarks are in video coordinate space; scale if needed
            const scaleX = v.overlay.width / (v.vid.videoWidth || v.vid.clientWidth || v.overlay.width);
            const scaleY = v.overlay.height / (v.vid.videoHeight || v.vid.clientHeight || v.overlay.height);
            const x = p.x * scaleX;
            const y = p.y * scaleY;
            if(i===0) ctx.moveTo(x,y);
            else ctx.lineTo(x,y);
          });
          ctx.closePath();
          ctx.stroke();

          // Take a high-quality crop (512x384) and compute metrics
          const crop = getEyeCrop(v.vid);
          const imgData = crop.getContext("2d").getImageData(0,0,crop.width,crop.height);

          // Placeholder/classical metrics (replace with real algorithm inputs if available)
          const P = computePrecision(120,3);
          const gamma = computeGamma(imgData);
          const depth = computeDepth(12,4);

          const hrVal = Number(v.hrEl.textContent) || 60;

          // Update shared state (lastMetrics)
          lastMetrics.gamma = gamma;
          lastMetrics.prec  = P;
          lastMetrics.depth = depth;
          lastMetrics.hr    = hrVal;
        }
      }catch(e){
        // Non-fatal: log but continue animation loop
        // console.debug("eyeLoop error:", e);
      }
    }
  }

  requestAnimationFrame(eyeLoop);
}
eyeLoop();

/* ===========================================================
   LIVE EQUATION + ALLUVIAL REFRESH (updates continuously)
=========================================================== */
function liveEquationLoop(){
  const hrVal = Number(hrFront.textContent) || lastMetrics.hr || 60;

  updateEquations(lastMetrics.gamma, lastMetrics.prec, lastMetrics.depth, hrVal);

  // Feed Alluvial with the same metrics divided by live PPG
  const ppg = Number(ppgValue.textContent) || 1;
  const vals = [
    lastMetrics.gamma / ppg,
    lastMetrics.prec  / ppg,
    lastMetrics.depth / ppg,
    (1/(hrVal || 1)) / ppg
  ];
  drawAlluvial(vals);

  requestAnimationFrame(liveEquationLoop);
}
liveEquationLoop();

/* ===========================================================
   HANDLE RESIZE/SCROLL (keep overlays aligned)
=========================================================== */
function refreshOverlays(){
  alignOverlay(camFront, frontOverlay);
  alignOverlay(camRear,  rearOverlay);
}
window.addEventListener('resize', refreshOverlays);
window.addEventListener('scroll', refreshOverlays);

/* ===========================================================
   Optional: show a small console hint if no camera permission
=========================================================== */
navigator.permissions && navigator.permissions.query({name:'camera'}).then((p)=>{
  if (p.state === 'denied') {
    console.warn("Camera permission denied. Please enable camera access in your browser to use the dashboard.");
  }
}).catch(()=>{/* ignore if unsupported */});

</script>

</body>
</html>
